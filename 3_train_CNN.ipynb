{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from file\n",
    "def load_file(filepath):\n",
    "\t\"\"\"\n",
    "\toutput: numpy array (samples, timesteps)\n",
    "\t\"\"\"\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array\n",
    "def load_group(filenames, prefix=''):\n",
    "\t\"\"\"\n",
    "\toutput: (samples, timesteps, features)\n",
    "\t\"\"\"\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group):\n",
    "\tfilepath = \"./UCI HAR Dataset/\"+ group + \"/Inertial Signals/\"\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file('./UCI HAR Dataset/' + group + '/y_'+group+'.txt')\n",
    "\treturn X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset():\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train')\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test')\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = tf.keras.utils.to_categorical(trainy)\n",
    "\ttesty = tf.keras.utils.to_categorical(testy)\n",
    "\treturn trainX, trainy, testX, testy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "def scale_data(trainX, testX):\n",
    "\t# remove overlap\n",
    "\tcut = int(trainX.shape[1] / 2)\n",
    "\tlongX = trainX[:, -cut:, :]\n",
    "\t# flatten windows\n",
    "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
    "\t# flatten train and test\n",
    "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
    "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
    "\t# standardize\n",
    "\ts = StandardScaler()\n",
    "\t# fit on training data\n",
    "\ts.fit(longX)\n",
    "\t# apply to training and test data\n",
    "\tlongX = s.transform(longX)\n",
    "\tflatTrainX = s.transform(flatTrainX)\n",
    "\tflatTestX = s.transform(flatTestX)\n",
    "\t# reshape\n",
    "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
    "\tflatTestX = flatTestX.reshape((testX.shape))\n",
    "\treturn flatTrainX, flatTestX\n",
    " \n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "\tverbose, epochs, batch_size = 0, 10, 32\n",
    "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\t# scale data\n",
    "\ttrainX, testX = scale_data(trainX, testX)\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=7, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=7, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu'))\n",
    "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy\n",
    " \n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\t# summarize mean and standard deviation\n",
    "\tfor i in range(len(scores)):\n",
    "\t\tm, s = np.mean(scores[i]), np.std(scores[i])\n",
    "\t\tprint('%.3f%% (+/-%.3f)' % (m, s))\n",
    " \n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# test each parameter\n",
    "\tall_scores = list()\n",
    "\t# repeat experiment\n",
    "\tscores = list()\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model(trainX, trainy, testX, testy)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\tall_scores.append(scores)\n",
    "\t# summarize results\n",
    "\tsummarize_results(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: 90.465\n",
      "#2: 91.245\n",
      "#3: 93.213\n",
      "#4: 92.637\n",
      "#5: 92.840\n",
      "#6: 90.227\n",
      "#7: 91.347\n",
      "#8: 91.924\n",
      "#9: 92.026\n",
      "#10: 91.992\n",
      "91.792% (+/-0.930)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.11"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "91.11"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c457c1bb00a46f16b86021fb5587bd66967ff67b33a9004ea71cf5283fcd661e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('GNNAL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
